---
layout: default
title: 11-4
lang: kr
lang-ref: 11-4
parent: Density Estimation with Gaussian Mixture Models
permalink: /kr/density-estimation-with-gaussian-mixture-models/11-4
nav_order: 4
---
# EM algorithm
{: .no_toc }


Density Estimation with Gaussian Mixture Models Chapter 3
{: .fs-5 .fw-300 }

---

{% include category.html category=page.parent id=3 %}

---

# Generative process와 latent vairables

## Latent variable이 포함된 generative model

Latent variable을 결정하는 분포 $\bold{\pi} = \left[ \pi_1, \pi_2, \cdots, \pi_K\right]^T$가 있다고 하자. 이전 단원에서는 데이터가 있을때 그것이 어떤 가우시안으로부터 왔다고 가정했지만, 이제는 그 데이터들이 어떤 latent space에 의해 영향을 받는다고 생각한다. 그래서 어떤 데이터 $\bold{x}$ 가 특정 latent variable $\bold{z}$에 생성되었을 때, 그것이 가우시안을 따를 확률은 $$p({\bold{x}, z_k = 1}) = p(\bold{x} \vert z_k = 1)p(z_k = 1) = \pi_k N(\bold{x}\vert \mu_k, \Sigma_k)$$
이다. 또한 이를 각각의 latent vairable에 대해 계산해 행렬 표현으로 나타낼 수도 있다.

## Likelihood

Likelihood를 구하는 방법으로 likelihood도 구할수 있다.
$$p(\bold{x} \vert \theta ) = \sum_{\bold{z}} p(\bold{x} \vert \theta, \bold{z}) p(\bold{z} \vert \theta) \\ = \sum_{k=1}^{K} p(\bold{x} \vert \theta, z_k = 1) p(z_k =1 \vert \theta) \\ = \sum_{k=1}^{K}\pi_k N(\bold{x} \vert \mu_k, \Sigma_k)$$
가 된다.

## Posterior Distribution

사후확률의 분포는 베이즈 정리를 이용하여 구할 수 있다.

## 전체 과정 살펴보기

$N$개의 데이터가 있다고 할 때, 각각의 데이터 포인트는 $K$개의 latent vairble를 가지고 있다. 그리고 이를 다음과 같이 표기한다.

$$\bold{z_n} = [z_{n1}, \cdots, z_{nK}]^T \in \mathbb{R}^K$$
라고 할 때, 각각의 latent variable이 i.i.d.라고 가정하면

$$p(\bold{x_1}, \cdots x_{N} \vert z_{1}, \cdots, z_{N}) = \prod_{n=1}^{N} p(\bold{x_n} \vert \bold{z_n})$$
이다.
또한 베이즈 정리를 통해 사후확률도 구할수 있고 이를 구하면 responsibility가 나온다.


## EM 알고리즘

이전 단원에서와 마찬가지로 E-step과 M-step을 번갈아가면서 진행되는데 E-step에서는 log-likelihood를 계산하게 되고 M-step에서는 모델 매개변수를 업데이트 하게 된다.

### E-step

현 시점 $t$에서의 모델 매개변수를 $\theta^{(t)}$라고 할때, posterior $p(\bold{z} \vert \bold{x}, \theta^{(t)})를 계산한다.$

### M-step
E-step에서 계산한 posterior를 가지고 $\log p(\bold{x}, \bold{z} \vert \theta)$의 기댓값을 구한다. 이는 적분을 이용하여 구할수 있고 (latent variable이 연속확률변수니까) 다음과 같다.
$$\int \log p(\bold{x}, \bold{z} \vert \theta) p(\bold{z} \vert \bold{x}, \theta^{(t)}) d\bold{z} \cdots (\rm{a})$$
(a)식을 최대화 하는 모델 매개변수 $\theta$를 다음 단계에서의 모델 매개변수로 업데이트한다.

여기서 유의해야 할 점은 EM 알고리즘이 반복됨에 따라 log-likelihood가 증가하기는 하지만, 이 알고리즘이 반드시 maximum likelihood를 구해주는 것은 아니라는 것이다. 극댓값을 구해줄 수 있기 때문에 모델 매개변수 $\theta$를 어떠한 방식으로 초기화 해 주느냐, 여러 초기화된 모델 매개변수를 가지고 여러번 알고리즘을 돌리느냐에 따라서 global maximum을 구할수도 있다.