---
layout: default
title: 11-3
lang: kr
lang-ref: 11-3
parent: Density Estimation with Gaussian Mixture Models
permalink: /kr/density-estimation-with-gaussian-mixture-models/11-3
nav_order: 3
---
# EM algorithm
{: .no_toc }


Density Estimation with Gaussian Mixture Models Chapter 3
{: .fs-5 .fw-300 }

---

{% include category.html category=page.parent id=3 %}

---

# EM 알고리즘

* Expectation maximization의 약자로 mixture model 일반화 해서 latent-variable 모델의 learning parameters(MAP)를 업데이트하는 iterative한 방법.

## 알고리즘 개요
1. $\mu_{k}, \Sigma_{k}, \pi_{k}$를 초기화 한다.
2. 모든 datapoint $x_n$에 대하여 responsibilities $r_{nk}$를 구한다. (Expectation)
3. $\pi_{k}, \mu_{k}, \Sigma_{k}$를 $r_{nk}$를 이용하여 업데이트 한다.
$$ \mu_k = \frac{1}{\sum r_{nk}} \sum_{n=1}^{N} r_{nk} x_n \\ \Sigma_k = \frac{1}{\sum r_{nk} \sum_{n=1}^{N} r_{nk}(x_{n} - \mu_{k}) (x_{n} - \mu_{k})^T} \\ \pi_k = \frac{\sum N_k}{N}$$ 

이를 간단한 예시를 들어 설명하면 다음과 같다.

1. 각각의 점들이 주어져 있고, 이들을 몇개의 가우시안들이 생성한 점이라고 가정한다.
2. (E-step) : 각각의 점에 대하여 각각의 가우시안들이 그 점을 생성했을때의 확률을 구한다.
3. (M-step) : 실제로 데이터들이 각각의 가우시안에 의해 생성되었다고 가정하고, 각 생성된 데이터에 대해 responsible한 가우시안의 확률이 최대가 되도록 parameter를 업데이트 한다. (가우시안의 경우 평균과 표준편차가 분포를 결정하므로 이를 업데이트 하면 된다.)