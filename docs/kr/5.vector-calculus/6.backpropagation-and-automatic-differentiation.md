---
layout: default
title: 역전파와 자동 미분
lang: kr
lang-ref: backpropagation-and-automatic-differentiation
parent: 벡터 미적분학
permalink: /kr/vector-calculus/5-6
nav_order: 6
writer: bluvory
---

# 역전파와 자동 미분
{: .no_toc }


Chapter 6 : Backpropagation and Automatic Differentiation
{: .fs-5 .fw-300 }


{% include writer.html writer=page.writer lang=page.lang %}

---

- 목차
    {: .text-gamma }

    1. TOC
    {:toc}

---

# 역전파와 자동 미분

수많은 머신러닝 어플리케이션에서는 gradient descent를 실행하기 위해 좋은 모델의 파라미터를 찾는 것이 중요하다. 이는 모델의 파라미터에 대한 학습 목표의 gradient를 연산한 데에 큰 영향을 미친다

우리는 section 5.2.2 에서 주어진 목적 함수에서 calculus를 통해 모델의 파라미터에 대한 gradient를 구하는 것을 배웠고, section 5.3에서는 linear regression의 파라미터에 대한 squared loss의 gradient를 구하는 것에 대해 간략하게 다루었다

하지만 이는 정확하지만 실용적인 방법은 아니다
deep neural network를 훈련할 때는 backpropagation algorithm을 사용한다
이는 모델 파라미터에 대한 error function의 gradient를 연산할 때보다 효율적이다


## 역전파 in multi layer perceptron

### notation 설명

$\textbf{w} = \{ w_{ij}^{(l)} \}$ : l번째 layer의 직전 layer에서 i번째 neuron과 연결된 j번째 neuron 의 weight

$\textbf{x}_{n}$ : training set에서의 n번째 example의 vectorized representation

$y_n$ : training set에서의 n 번째 example의 label

$s_{j}^{(l)}$ : l번째 layer의 j번째 neuron에서 activation 직전까지 나온 값

$d^{l}$ : l번째 layer의 neuron의 개수

$e(h(\textbf{x_n}, y_{n})) = e(\textbf{w})$ : error

### 역전파 과정 설명

SGD를 이용한 MLP의 backprop 설명

최종 목적은 $\nabla e(\textbf{w})$을 구하는 것!

### delta rule

$\delta_{j}^{(l)} = \frac{\partial e}{\partial {s_{j}^{(l)}}}$ 라고 생각한다면,

$$\delta_{i}^{(l-1)} = \frac{\partial e}{\partial {s_{i}^{(l - 1)}}} \\
                     = \sum_{j=1}^{d^{(l)}} \frac{\partial e}{\partial {s_{j}^{(l)}}} \times \frac{\partial {s_{j}^{(l)}}}{\partial {x_{i}^{(l - 1)}}} \times \frac{\partial x_{i}^{(l - 1)}}{\partial {s_{i}^{(l - 1)}}} \\
                     = \sum_{j=1}^{d^{(l)}} \delta_{j}^{(l)} \times w_{ij}^{(l)} \times \theta'(s_{i}^{(l - 1)})$$

### Backprop algorithm with SGD

1. $w_{ij}^{(l)}$ 들을 초기화 한다.
2. for $t = 0, 1, 2, \cdots, $ do
3. 1~$N$에서 $n$을 고른다.
4. $x_{j}^{(l)}$ 를 전부 계산한다(forward)
5. $\delta_{j}^{(l)}$ 을 전부 계산한다. (backward)
6. $w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \eta x_{i}^{(l - 1)}\delta_{j}^{(l)}$
7. 모든 과정이 끝날때 까지 반복한다.



## 5.6.2 Automatic differentiation

### Symbolic differentiation vs Automatic differentiation

Symbolic differentiation은 우리가 손으로 평소에 하는 미분을 의미한다. 그것과 다르게 automatic differentiation은 중간 단계의 변수와 chain rule을 사용해서 수치적으로 미분을 계산하는 것을 말한다. 위에서 본 backprop 과정에서의 delta를 구하는 과정도 결국에는 chain rule을 이용했기 때문에 automatic differentiation의 일종이라고 할 수 있다. 
---

{% include category.html category=page.parent id=6 %}
